{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCHbLkgUbF8IZyHlhYsRNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhwnoh/UST-GenerativeModels/blob/main/P2_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define Data,Model,Sampling"
      ],
      "metadata": {
        "id": "lYuKZhsH5wJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install rdkit & convert SMILES to molecules"
      ],
      "metadata": {
        "id": "shbxfuR55qXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_C8SaD3xBy",
        "outputId": "57f77ab0-4dbd-4fd8-ab5a-509d24ee2483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "\n",
        "import rdkit\n",
        "from rdkit.Chem import MolFromSmiles,MolToSmiles\n",
        "import rdkit\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define required packages"
      ],
      "metadata": {
        "id": "Hk0RIjR09xLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "euXKCmUn9wiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MolData(Dataset):\n",
        "  def __init__(self,smis,toks):\n",
        "    self.smis = smis\n",
        "    self.toks = toks + ['<','>'] #'<'; start of sequence, '>'; end of sequence\n",
        "    self.Ntok = len(toks)\n",
        "    self.Nmax = 120\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.smis)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    smi = '<'+self.smis[idx]+'>'\n",
        "    smi += '>'*(self.Nmax-len(smi))\n",
        "\n",
        "    x_all = np.array([self.toks.index(s) for s in smi]).flatten()\n",
        "    y = x_all[1:] #output\n",
        "\n",
        "    x = torch.LongTensor(x_all)\n",
        "    y = torch.LongTensor(y)\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "QBlZUpPY-hV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MolVAE(nn.Module):\n",
        "    def __init__(self,dim_x0,dim_x1,dim_h,n_layer,d_ratio,dim_z):\n",
        "        super(MolVAE,self).__init__()\n",
        "        self.n_layer = n_layer\n",
        "        self.emb_layer = nn.Embedding(dim_x0,dim_x1)\n",
        "\n",
        "        self.enc = nn.GRU(dim_x1,dim_h,\n",
        "                        num_layers=n_layer,\n",
        "                        dropout = d_ratio,\n",
        "                        batch_first = True)\n",
        "\n",
        "        self.fc_z1 = nn.Sequential(\n",
        "                            nn.Linear(dim_h,dim_h),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(dim_h,2*dim_z))\n",
        "\n",
        "        self.fc_z2 = nn.Linear(dim_z,dim_h)\n",
        "\n",
        "        self.dec = nn.GRU(dim_x1+dim_z,dim_h,\n",
        "                        num_layers=n_layer,\n",
        "                        dropout = d_ratio,\n",
        "                        batch_first = True)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "                  nn.Linear(dim_h,dim_h),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(dim_h,dim_x0))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_emb = self.emb_layer(x)\n",
        "\n",
        "        mu,log_var = self.encoder(x_emb)\n",
        "        eps = torch.randn_like(mu)\n",
        "        z = mu + eps*torch.exp(log_var/2)\n",
        "\n",
        "        out = self.decoder(x_emb[:,:-1],z)\n",
        "        return out,mu,log_var\n",
        "\n",
        "    def encoder(self,x):\n",
        "        _,h1 = self.enc(x,None)\n",
        "        h2 = self.fc_z1(h1[-1])\n",
        "        mu,log_var = torch.chunk(h2,2,dim=-1)\n",
        "        return mu,log_var\n",
        "\n",
        "    def decoder(self,x,z):\n",
        "        N,L,F = x.shape\n",
        "        h0_z = z.unsqueeze(1).repeat(1,L,1)\n",
        "\n",
        "        x_in = torch.cat([x,h0_z],dim=-1)\n",
        "\n",
        "        h0_rnn = self.fc_z2(z).unsqueeze(0).repeat(self.n_layer,1,1)\n",
        "        out,h_d = self.dec(x_in,h0_rnn)\n",
        "        out = self.out(out)\n",
        "        return out\n",
        "\n",
        "    def sampling(self,x0,z,h0=None,is_first=True):\n",
        "        x = self.emb_layer(x0)\n",
        "\n",
        "        N,L,F = x.shape\n",
        "        h0_z = z.unsqueeze(1).repeat(1,L,1)\n",
        "        x_in = torch.cat([x,h0_z],dim=-1)\n",
        "\n",
        "        if is_first:\n",
        "            h0 = self.fc_z2(z).unsqueeze(0).repeat(self.n_layer,1,1)\n",
        "\n",
        "        out,h1 = self.dec(x_in,h0)\n",
        "        out = self.out(out)\n",
        "        return out,h1"
      ],
      "metadata": {
        "id": "DPuk4LZt-mjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Sampling(sampler,dim_z,n_sample,max_len,tok_lib):\n",
        "    sampler.eval()\n",
        "    with torch.no_grad():\n",
        "        inits = torch.LongTensor([34]*n_sample)\n",
        "        loader = DataLoader(inits,batch_size=100)\n",
        "\n",
        "        Sampled = []\n",
        "        Zs = []\n",
        "        for inp in tqdm(loader):\n",
        "            x_in = inp.reshape(-1,1)\n",
        "\n",
        "            x_hat = []\n",
        "            z = torch.randn(len(x_in),dim_z)\n",
        "            h = None\n",
        "            is_first = True\n",
        "            for seq_iter in range(max_len):\n",
        "\n",
        "                if seq_iter > 0:\n",
        "                    is_first = False\n",
        "\n",
        "                out,h = sampler.sampling(x_in,z,h,is_first)\n",
        "                prob = F.softmax(out,dim=-1).squeeze(1)\n",
        "                x_in = torch.multinomial(prob,1)\n",
        "\n",
        "                x_hat.append(x_in.cpu().detach().numpy())\n",
        "\n",
        "            x_hat = np.hstack(x_hat)\n",
        "            Sampled.append(x_hat)\n",
        "            Zs.append(z.cpu().detach().numpy())\n",
        "\n",
        "        Sampled = np.vstack(Sampled)\n",
        "        Zs = np.vstack(Zs)\n",
        "\n",
        "        Mols = []\n",
        "        Idx = []\n",
        "        for i,s in enumerate(Sampled):\n",
        "            n_end = np.sum(s==35)\n",
        "\n",
        "            if n_end == 0:\n",
        "                continue\n",
        "\n",
        "            n = np.min(np.where(s==35)[0])\n",
        "            m = ''.join(tok_lib[s[:n]].tolist())\n",
        "            Mols.append(m)\n",
        "            Idx.append(i)\n",
        "\n",
        "        Vals = []\n",
        "        Lat = []\n",
        "        for ii in Idx:\n",
        "            smi = Mols[ii]\n",
        "            mol = MolFromSmiles(smi)\n",
        "            if not mol is None:\n",
        "                Vals.append(MolToSmiles(mol))\n",
        "                Lat.append(Zs[ii])\n",
        "\n",
        "        Uni = list(set(Vals))\n",
        "        return Vals,Lat,len(Vals),len(Uni)"
      ],
      "metadata": {
        "id": "8Gcd5GJ5AN0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Trainer"
      ],
      "metadata": {
        "id": "tC3jpfsLAwKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearAnnealing(n_iter, start=0.0, stop=1.0,  n_cycle=4, ratio=0.5):\n",
        "    L = np.ones(n_iter) * stop\n",
        "    period = n_iter/n_cycle\n",
        "    step = (stop-start)/(period*ratio) # linear schedule\n",
        "\n",
        "    for c in range(n_cycle):\n",
        "        v, i = start, 0\n",
        "        while v <= stop and (int(i+c*period) < n_iter):\n",
        "            L[int(i+c*period)] = v\n",
        "            v += step\n",
        "            i += 1\n",
        "    return L"
      ],
      "metadata": {
        "id": "OPKUXp-XA9PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/main/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv')\n",
        "smis_ = [ss.split()[0] for ss in df['smiles']]\n",
        "\n",
        "toks = []\n",
        "for smi in tqdm(smis_):\n",
        "  toks += list(set(smi))\n",
        "  toks = list(set(toks))\n",
        "toks = list(set(toks))\n",
        "\n",
        "n_train = 1000\n",
        "n_val = 1000\n",
        "\n",
        "np.random.seed(1)\n",
        "np.random.shuffle(smis_)\n",
        "\n",
        "smi_train = smis_[:n_train]\n",
        "smi_val = smis_[n_train:n_train+n_val]\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_data = MolData(smi_train,toks)\n",
        "tok_lib = np.array(train_data.toks) # For sampling\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "test_data = MolData(smi_val,toks)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "DimZ = 156\n",
        "model = MolVAE(36,64,256,2,0.2,DimZ)\n",
        "\n",
        "lr = 2e-4\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "num_epoch = 200\n",
        "max_norm = 5\n",
        "\n",
        "LOGs = []\n",
        "Betas = LinearAnnealing(n_iter=num_epoch,start=0.0,stop=0.2).tolist()\n",
        "\n",
        "for ep in range(num_epoch):\n",
        "    model.train()\n",
        "    for inp in tqdm(train_loader):\n",
        "        x_in = inp[0]\n",
        "        tgt = inp[1].view(-1)\n",
        "\n",
        "        x_out,mu,log_var = model(x_in)\n",
        "\n",
        "        rec = ce_loss(x_out.reshape(-1,36),tgt)\n",
        "        kld = torch.mean(0.5*(mu**2+torch.exp(log_var)-log_var-1))\n",
        "\n",
        "        loss = rec + Betas[ep]*kld\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Sim = []\n",
        "        Mus = []\n",
        "        Stds = []\n",
        "        KLDs = 0\n",
        "        Ns = 0\n",
        "        for inp in tqdm(test_loader):\n",
        "            x_in = inp[0]\n",
        "            tgt = inp[1].view(-1)\n",
        "\n",
        "            x_out,mu,log_var = model(x_in)\n",
        "\n",
        "            kld = torch.sum(torch.mean(0.5*(mu**2+torch.exp(log_var)-log_var-1),-1))\n",
        "\n",
        "            KLDs += kld.cpu().detach().numpy().flatten()[0]\n",
        "            Ns += len(x_in)\n",
        "\n",
        "            id_out = np.argmax(x_out.cpu().detach().numpy(),-1)\n",
        "            id_in = x_in[:,1:].cpu().detach().numpy()\n",
        "            acc = np.mean(id_out==id_in,1).reshape(-1,1)\n",
        "\n",
        "            Sim.append(acc)\n",
        "            Mus.append(mu.cpu().detach().numpy())\n",
        "            Stds.append(torch.exp(log_var/2).cpu().detach().numpy())\n",
        "\n",
        "        Sim = np.vstack(Sim)\n",
        "        Mus = np.vstack(Mus)\n",
        "        Stds = np.vstack(Stds)\n",
        "        mols,z_mol,val,uniq = Sampling(model,DimZ,1000,100,tok_lib)\n",
        "\n",
        "        print(ep,Betas[ep],np.min(Sim),np.max(Sim),np.mean(Sim),np.std(Sim),KLDs/Ns,val,uniq)\n"
      ],
      "metadata": {
        "id": "0Me0vX-44TTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Use pre-trained model"
      ],
      "metadata": {
        "id": "BGavfzVaD5hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DimZ = 156\n",
        "model = MolVAE(36,128,480,3,0.2,DimZ)\n",
        "\n",
        "chkpt = torch.load('your/path',map_location='cpu')\n",
        "model.load_state_dict(model['state_dict'])\n",
        "\n",
        "mols,z_mol,val,uniq = Sampling(model,DimZ,1000,100,tok_lib)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p009FyfoCX9h",
        "outputId": "0ca7d400-5244-41c2-f362-93688e82d791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1584663/1584663 [00:04<00:00, 319626.70it/s]\n",
            "100%|██████████| 10/10 [00:21<00:00,  2.10s/it]\n"
          ]
        }
      ]
    }
  ]
}